{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "了解如何使用 Sklearn 中的 hyper-parameter search 找出最佳的超參數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業\n",
    "請使用不同的資料集，並使用 hyper-parameter search 的方式，看能不能找出最佳的超參數組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先載入套件和資料集\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用隨機森林:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9577777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred1 = clf.predict(x_test)\n",
    "\n",
    "acc1 = metrics.accuracy_score(y_test, y_pred1)\n",
    "print(\"Acuuracy: \", acc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Grid Search對RF進行調參:\n",
    "\n",
    "Hyperparameter Tuning the Random Forest in Python:\n",
    "\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'class_weight': None,\n",
      " 'criterion': 'gini',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': None,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(clf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "# Random Hyperparameter Grid\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! \n",
    "\n",
    "However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   21.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Search Training\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 100,\n",
    "                               cv = 3,\n",
    "                               verbose=2,\n",
    "                               random_state=42,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 400,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': None,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model's Acuuracy:  0.9555555555555556\n",
      "===========================================\n",
      "Best model's Acuuracy:  0.9822222222222222\n",
      "===========================================\n",
      "Improvement of 2.79%.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Random Search\n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "best_random = rf_random.best_estimator_\n",
    "\n",
    "base_model.fit(x_train,y_train)\n",
    "y_pred2 = base_model.predict(x_test)\n",
    "acc2 = metrics.accuracy_score(y_test, y_pred2)\n",
    "print(\"Base model's Acuuracy: \", acc2)\n",
    "\n",
    "print(\"===========================================\")\n",
    "\n",
    "best_random.fit(x_train,y_train)\n",
    "y_pred3 = best_random.predict(x_test)\n",
    "acc3 = metrics.accuracy_score(y_test, y_pred3)\n",
    "print(\"Best model's Acuuracy: \", acc3)\n",
    "\n",
    "print(\"===========================================\")\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format(100*(acc3-acc2)/acc2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search with Cross Validation\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf2 = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf2,\n",
    "                           param_grid = param_grid, \n",
    "                           cv = 3,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 2)\n",
    "\n",
    "# This will try out 1 * 4 * 2 * 3 * 3 * 4 = 288 combinations of settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   46.6s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 100,\n",
       " 'max_features': 3,\n",
       " 'min_samples_leaf': 3,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(x_train,y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model's Acuuracy:  0.9555555555555556\n",
      "===========================================\n",
      "Best grid's Acuuracy:  0.9622222222222222\n",
      "===================================================\n",
      "Improvement of 0.70%.\n"
     ]
    }
   ],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "base_model.fit(x_train,y_train)\n",
    "y_pred2 = base_model.predict(x_test)\n",
    "acc2 = metrics.accuracy_score(y_test, y_pred2)\n",
    "print(\"Base model's Acuuracy: \", acc2)\n",
    "\n",
    "print(\"===========================================\")\n",
    "\n",
    "best_grid.fit(x_train,y_train)\n",
    "y_pred4 = best_grid.predict(x_test)\n",
    "acc4 = metrics.accuracy_score(y_test, y_pred4)\n",
    "\n",
    "print(\"Best grid's Acuuracy: \", acc4)\n",
    "\n",
    "print(\"===================================================\")\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format(100*(acc4-acc2)/acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "隨機grid search 的效果似乎比純粹grid search要來得更好~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用GBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9688888888888889\n"
     ]
    }
   ],
   "source": [
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=42)\n",
    "\n",
    "# 建立模型\n",
    "gbm = GradientBoostingClassifier(random_state=7)\n",
    "\n",
    "gbm.fit(x_train, y_train)\n",
    "\n",
    "y_pred5 = gbm.predict(x_test)\n",
    "\n",
    "acc5 = metrics.accuracy_score(y_test, y_pred5)\n",
    "print(\"Acuuracy: \", acc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='auto',\n",
       "                           random_state=7, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Grid Search對GBM進行調參:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:   24.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "# 設定要訓練的超參數組合\n",
    "n_estimators = [100, 200, 300]\n",
    "max_depth = [1, 3, 5]\n",
    "param_grid = dict(n_estimators=n_estimators,\n",
    "                  max_depth=max_depth)\n",
    "\n",
    "## 建立搜尋物件，放入模型及參數組合字典 (n_jobs=-1 會使用全部 cpu 平行運算)\n",
    "grid_search = GridSearchCV(gbm,\n",
    "                           param_grid,\n",
    "                           scoring=\"accuracy\",\n",
    "                           # https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# 開始搜尋最佳參數\n",
    "grid_result = grid_search.fit(x_train, y_train)\n",
    "\n",
    "# 預設會跑 3-fold cross-validadtion，總共 9 種參數組合，總共要 train 27 次模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy: 0.947290 using {'max_depth': 3, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "# 印出最佳結果與最佳參數\n",
    "print(\"Best Accuracy: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'n_estimators': 200}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9711111111111111\n",
      "===================================================\n",
      "Improvement of 0.23%.\n"
     ]
    }
   ],
   "source": [
    "# 使用最佳參數重新建立模型\n",
    "gbm_bestparam = GradientBoostingClassifier(max_depth=grid_result.best_params_['max_depth'],\n",
    "                                           n_estimators=grid_result.best_params_['n_estimators'])\n",
    "\n",
    "# 訓練模型\n",
    "gbm_bestparam.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred6 = gbm_bestparam.predict(x_test)\n",
    "\n",
    "acc6 = metrics.accuracy_score(y_test, y_pred6)\n",
    "print(\"Acuuracy: \", acc6)\n",
    "\n",
    "print(\"===================================================\")\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format(100*(acc6-acc5)/acc5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "調整之後正確率從0.9688上升至0.9711了!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用XGBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9711111111111111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    " \n",
    "digits = datasets.load_digits()\n",
    " \n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "xgb = xgb.XGBClassifier()\n",
    "\n",
    "# 訓練模型\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred7 = xgb.predict(x_test)\n",
    "\n",
    "acc7 = metrics.accuracy_score(y_test, y_pred7)\n",
    "print(\"Acuuracy: \", acc7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用Grid Search對XGBoost進行調參:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "from sklearn import datasets, metrics\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "xgb2 = xgb.XGBClassifier()\n",
    "\n",
    "param_dist = {\n",
    "        'n_estimators':range(80,200,4),\n",
    "        'max_depth':range(2,15,1),\n",
    "        'learning_rate':np.linspace(0.01,2,20),\n",
    "        'subsample':np.linspace(0.7,0.9,20),\n",
    "        'colsample_bytree':np.linspace(0.5,0.98,10),\n",
    "        'min_child_weight':range(1,9,1)\n",
    "        }#跑不動啦幹3777萬個fits\n",
    "\n",
    "param_grid = {\n",
    "        'n_estimators':range(80,200,10),\n",
    "        'max_depth':range(2,15,1),\n",
    "        'learning_rate':np.linspace(0.01,0.5,10),\n",
    "        #'subsample':np.linspace(0.7,0.9,10),\n",
    "        #'colsample_bytree':np.linspace(0.5,0.98,10),\n",
    "        #'min_child_weight':range(1,9,1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1560 candidates, totalling 4680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  6.6min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed: 16.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 29.8min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 34.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4680 out of 4680 | elapsed: 38.9min finished\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 開始搜尋最佳參數\n",
    "grid_result = GridSearchCV(xgb2,\n",
    "                    param_grid,\n",
    "                    scoring=\"accuracy\",\n",
    "                    cv = 3,\n",
    "                    #n_iter=300, #RandomizedSearchCV才有此參數\n",
    "                    n_jobs = -1,\n",
    "                    verbose=1)\n",
    "\n",
    " \n",
    "grid_result = grid_result.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.3911111111111111, max_delta_step=0, max_depth=2,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=110, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "best_estimator = grid_result.best_estimator_\n",
    "print(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's Acuuracy:  0.9822222222222222\n",
      "=======================================\n",
      "Improvement of 1.14%.\n"
     ]
    }
   ],
   "source": [
    "y_pred8 = grid_result.predict(x_test)\n",
    "\n",
    "acc8 = metrics.accuracy_score(y_test, y_pred8)\n",
    "print(\"Best model's Acuuracy: \", acc8)\n",
    "\n",
    "print(\"=======================================\")\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format(100*(acc8-acc7)/acc7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
